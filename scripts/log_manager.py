#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥å¿—ç®¡ç†è„šæœ¬
æä¾›æ—¥å¿—æŸ¥çœ‹ã€æ¸…ç†ã€åˆ†æç­‰åŠŸèƒ½
"""

import os
import sys
import json
import argparse
from datetime import datetime, timedelta
from pathlib import Path
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from utils.logger import get_logger

class LogManager:
    """æ—¥å¿—ç®¡ç†å™¨"""
    
    def __init__(self, log_dir='logs'):
        self.log_dir = Path(log_dir)
        self.logger = get_logger('log_manager')
        
        if not self.log_dir.exists():
            self.logger.warning(f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {self.log_dir}")
            self.log_dir.mkdir(parents=True, exist_ok=True)
    
    def list_log_files(self):
        """åˆ—å‡ºæ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
        log_files = []
        
        for file_path in self.log_dir.rglob('*.log*'):
            if file_path.is_file():
                stat = file_path.stat()
                log_files.append({
                    'name': file_path.name,
                    'path': str(file_path),
                    'size': stat.st_size,
                    'modified': datetime.fromtimestamp(stat.st_mtime),
                    'size_mb': round(stat.st_size / (1024 * 1024), 2)
                })
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        log_files.sort(key=lambda x: x['modified'], reverse=True)
        return log_files
    
    def show_log_summary(self):
        """æ˜¾ç¤ºæ—¥å¿—æ‘˜è¦ä¿¡æ¯"""
        log_files = self.list_log_files()
        
        if not log_files:
            print("ğŸ“ æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
            return
        
        total_size = sum(f['size'] for f in log_files)
        total_files = len(log_files)
        
        print(f"ğŸ“Š æ—¥å¿—æ‘˜è¦ ({self.log_dir})")
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files}")
        print(f"ğŸ’¾ æ€»å¤§å°: {round(total_size / (1024 * 1024), 2)} MB")
        print()
        
        print("ğŸ“‹ æ—¥å¿—æ–‡ä»¶åˆ—è¡¨:")
        for file_info in log_files:
            status = "ğŸŸ¢" if file_info['size'] < 10 * 1024 * 1024 else "ğŸŸ¡"  # 10MB
            if file_info['size'] > 100 * 1024 * 1024:  # 100MB
                status = "ğŸ”´"
            
            print(f"  {status} {file_info['name']}")
            print(f"     å¤§å°: {file_info['size_mb']} MB")
            print(f"     ä¿®æ”¹: {file_info['modified'].strftime('%Y-%m-%d %H:%M:%S')}")
            print()
    
    def view_log_file(self, filename, lines=50, follow=False, filter_level=None):
        """æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶å†…å®¹"""
        file_path = self.log_dir / filename
        
        if not file_path.exists():
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            return
        
        print(f"ğŸ“– æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: {filename}")
        print(f"ğŸ“ è·¯å¾„: {file_path}")
        print(f"ğŸ“ å¤§å°: {round(file_path.stat().st_size / 1024, 2)} KB")
        print("-" * 80)
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                # è¯»å–æœ€åå‡ è¡Œ
                all_lines = f.readlines()
                start_line = max(0, len(all_lines) - lines)
                
                for i, line in enumerate(all_lines[start_line:], start_line + 1):
                    if self._should_display_line(line, filter_level):
                        print(f"{i:6d}: {line.rstrip()}")
                
                if follow:
                    print("\nğŸ”„ å®æ—¶ç›‘æ§æ¨¡å¼ (æŒ‰ Ctrl+C åœæ­¢)")
                    try:
                        while True:
                            new_lines = f.readlines()
                            for line in new_lines:
                                if self._should_display_line(line, filter_level):
                                    print(f"NEW: {line.rstrip()}")
                    except KeyboardInterrupt:
                        print("\nâ¹ï¸  åœæ­¢ç›‘æ§")
        
        except Exception as e:
            print(f"âŒ è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
    
    def _should_display_line(self, line, filter_level):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ˜¾ç¤ºè¯¥è¡Œ"""
        if not filter_level:
            return True
        
        # å°è¯•è§£æJSONæ ¼å¼çš„æ—¥å¿—
        try:
            log_data = json.loads(line.strip())
            if 'level' in log_data:
                return log_data['level'].upper() == filter_level.upper()
        except:
            pass
        
        # ä¼ ç»Ÿæ ¼å¼æ—¥å¿—
        level_pattern = rf'\b{filter_level.upper()}\b'
        return re.search(level_pattern, line, re.IGNORECASE) is not None
    
    def search_logs(self, query, files=None, case_sensitive=False):
        """æœç´¢æ—¥å¿—å†…å®¹"""
        if files is None:
            files = [f.name for f in self.log_dir.glob('*.log')]
        
        results = []
        flags = 0 if case_sensitive else re.IGNORECASE
        
        for filename in files:
            file_path = self.log_dir / filename
            if not file_path.exists():
                continue
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        if re.search(query, line, flags):
                            results.append({
                                'file': filename,
                                'line': line_num,
                                'content': line.strip(),
                                'timestamp': self._extract_timestamp(line)
                            })
            except Exception as e:
                print(f"âš ï¸  è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {e}")
        
        # æŒ‰æ—¶é—´æ’åº
        results.sort(key=lambda x: x['timestamp'] or datetime.min)
        
        print(f"ğŸ” æœç´¢ç»“æœ: '{query}'")
        print(f"ğŸ“Š æ‰¾åˆ° {len(results)} æ¡åŒ¹é…è®°å½•")
        print("-" * 80)
        
        for result in results:
            print(f"ğŸ“„ {result['file']}:{result['line']}")
            print(f"â° {result['timestamp'] or 'Unknown'}")
            print(f"ğŸ“ {result['content']}")
            print("-" * 40)
    
    def _extract_timestamp(self, line):
        """ä»æ—¥å¿—è¡Œä¸­æå–æ—¶é—´æˆ³"""
        # JSONæ ¼å¼
        try:
            log_data = json.loads(line.strip())
            if 'timestamp' in log_data:
                return datetime.fromisoformat(log_data['timestamp'].replace('Z', '+00:00'))
        except:
            pass
        
        # ä¼ ç»Ÿæ ¼å¼
        timestamp_patterns = [
            r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})',
            r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})',
        ]
        
        for pattern in timestamp_patterns:
            match = re.search(pattern, line)
            if match:
                try:
                    return datetime.strptime(match.group(1), '%Y-%m-%d %H:%M:%S')
                except:
                    try:
                        return datetime.fromisoformat(match.group(1))
                    except:
                        pass
        
        return None
    
    def clean_old_logs(self, days=30, dry_run=True):
        """æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶"""
        cutoff_date = datetime.now() - timedelta(days=days)
        old_files = []
        
        for file_path in self.log_dir.rglob('*.log*'):
            if file_path.is_file():
                mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
                if mtime < cutoff_date:
                    old_files.append({
                        'path': file_path,
                        'size': file_path.stat().st_size,
                        'modified': mtime
                    })
        
        if not old_files:
            print("âœ¨ æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ—§æ—¥å¿—æ–‡ä»¶")
            return
        
        total_size = sum(f['size'] for f in old_files)
        print(f"ğŸ—‘ï¸  æ‰¾åˆ° {len(old_files)} ä¸ªæ—§æ—¥å¿—æ–‡ä»¶")
        print(f"ğŸ’¾ æ€»å¤§å°: {round(total_size / (1024 * 1024), 2)} MB")
        print(f"ğŸ“… æ¸…ç† {days} å¤©å‰çš„æ–‡ä»¶")
        print()
        
        for file_info in old_files:
            print(f"  ğŸ“„ {file_info['path'].name}")
            print(f"     å¤§å°: {round(file_info['size'] / 1024, 2)} KB")
            print(f"     ä¿®æ”¹: {file_info['modified'].strftime('%Y-%m-%d')}")
        
        if not dry_run:
            print(f"\nğŸ—‘ï¸  æ­£åœ¨åˆ é™¤ {len(old_files)} ä¸ªæ–‡ä»¶...")
            for file_info in old_files:
                try:
                    file_info['path'].unlink()
                    print(f"  âœ… å·²åˆ é™¤: {file_info['path'].name}")
                except Exception as e:
                    print(f"  âŒ åˆ é™¤å¤±è´¥: {file_info['path'].name} - {e}")
            
            print(f"\nâœ¨ æ¸…ç†å®Œæˆï¼Œé‡Šæ”¾ç©ºé—´: {round(total_size / (1024 * 1024), 2)} MB")
        else:
            print(f"\nğŸ” è¿™æ˜¯é¢„è§ˆæ¨¡å¼ï¼Œä½¿ç”¨ --execute å‚æ•°æ‰§è¡Œå®é™…æ¸…ç†")
    
    def analyze_logs(self, filename, hours=24):
        """åˆ†ææ—¥å¿—æ–‡ä»¶"""
        file_path = self.log_dir / filename
        
        if not file_path.exists():
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            return
        
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_lines': 0,
            'level_counts': {},
            'error_count': 0,
            'warning_count': 0,
            'info_count': 0,
            'debug_count': 0,
            'time_distribution': {},
            'top_errors': []
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    stats['total_lines'] += 1
                    
                    # è§£ææ—¥å¿—çº§åˆ«
                    level = self._extract_log_level(line)
                    if level:
                        stats['level_counts'][level] = stats['level_counts'].get(level, 0) + 1
                        
                        if level == 'ERROR':
                            stats['error_count'] += 1
                        elif level == 'WARNING':
                            stats['warning_count'] += 1
                        elif level == 'INFO':
                            stats['info_count'] += 1
                        elif level == 'DEBUG':
                            stats['debug_count'] += 1
                    
                    # æ—¶é—´åˆ†å¸ƒ
                    timestamp = self._extract_timestamp(line)
                    if timestamp and timestamp >= cutoff_time:
                        hour = timestamp.strftime('%Y-%m-%d %H:00')
                        stats['time_distribution'][hour] = stats['time_distribution'].get(hour, 0) + 1
                    
                    # æ”¶é›†é”™è¯¯ä¿¡æ¯
                    if level == 'ERROR':
                        error_msg = self._extract_error_message(line)
                        if error_msg:
                            stats['top_errors'].append(error_msg)
        
        except Exception as e:
            print(f"âŒ åˆ†ææ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
            return
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        print(f"ğŸ“Š æ—¥å¿—åˆ†ææŠ¥å‘Š: {filename}")
        print(f"â° åˆ†ææ—¶é—´èŒƒå›´: æœ€è¿‘ {hours} å°æ—¶")
        print(f"ğŸ“ æ€»è¡Œæ•°: {stats['total_lines']}")
        print()
        
        print("ğŸ“ˆ æ—¥å¿—çº§åˆ«ç»Ÿè®¡:")
        for level, count in sorted(stats['level_counts'].items()):
            percentage = (count / stats['total_lines']) * 100
            print(f"  {level}: {count} ({percentage:.1f}%)")
        print()
        
        print("â° æ—¶é—´åˆ†å¸ƒ (æœ€è¿‘24å°æ—¶):")
        for hour, count in sorted(stats['time_distribution'].items()):
            print(f"  {hour}: {count} æ¡")
        print()
        
        if stats['top_errors']:
            print("ğŸš¨ å¸¸è§é”™è¯¯ (å‰10ä¸ª):")
            error_counts = {}
            for error in stats['top_errors']:
                error_counts[error] = error_counts.get(error, 0) + 1
            
            for error, count in sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {count}x: {error}")
    
    def _extract_log_level(self, line):
        """ä»æ—¥å¿—è¡Œä¸­æå–æ—¥å¿—çº§åˆ«"""
        # JSONæ ¼å¼
        try:
            log_data = json.loads(line.strip())
            if 'level' in log_data:
                return log_data['level'].upper()
        except:
            pass
        
        # ä¼ ç»Ÿæ ¼å¼
        level_pattern = r'\b(DEBUG|INFO|WARNING|ERROR|CRITICAL)\b'
        match = re.search(level_pattern, line, re.IGNORECASE)
        return match.group(1).upper() if match else None
    
    def _extract_error_message(self, line):
        """ä»æ—¥å¿—è¡Œä¸­æå–é”™è¯¯æ¶ˆæ¯"""
        # JSONæ ¼å¼
        try:
            log_data = json.loads(line.strip())
            if 'message' in log_data:
                return log_data['message'][:100]  # é™åˆ¶é•¿åº¦
        except:
            pass
        
        # ä¼ ç»Ÿæ ¼å¼
        return line.strip()[:100]

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ—¥å¿—ç®¡ç†å·¥å…·')
    parser.add_argument('--log-dir', default='logs', help='æ—¥å¿—ç›®å½•è·¯å¾„')
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # åˆ—å‡ºæ—¥å¿—æ–‡ä»¶
    subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰æ—¥å¿—æ–‡ä»¶')
    
    # æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶
    view_parser = subparsers.add_parser('view', help='æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶å†…å®¹')
    view_parser.add_argument('filename', help='æ—¥å¿—æ–‡ä»¶å')
    view_parser.add_argument('-n', '--lines', type=int, default=50, help='æ˜¾ç¤ºè¡Œæ•°')
    view_parser.add_argument('-f', '--follow', action='store_true', help='å®æ—¶ç›‘æ§')
    view_parser.add_argument('--level', help='è¿‡æ»¤æ—¥å¿—çº§åˆ«')
    
    # æœç´¢æ—¥å¿—
    search_parser = subparsers.add_parser('search', help='æœç´¢æ—¥å¿—å†…å®¹')
    search_parser.add_argument('query', help='æœç´¢æŸ¥è¯¢')
    search_parser.add_argument('--files', nargs='+', help='æŒ‡å®šæ—¥å¿—æ–‡ä»¶')
    search_parser.add_argument('--case-sensitive', action='store_true', help='åŒºåˆ†å¤§å°å†™')
    
    # æ¸…ç†æ—§æ—¥å¿—
    clean_parser = subparsers.add_parser('clean', help='æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶')
    clean_parser.add_argument('--days', type=int, default=30, help='ä¿ç•™å¤©æ•°')
    clean_parser.add_argument('--execute', action='store_true', help='æ‰§è¡Œå®é™…æ¸…ç†')
    
    # åˆ†ææ—¥å¿—
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†ææ—¥å¿—æ–‡ä»¶')
    analyze_parser.add_argument('filename', help='æ—¥å¿—æ–‡ä»¶å')
    analyze_parser.add_argument('--hours', type=int, default=24, help='åˆ†ææ—¶é—´èŒƒå›´(å°æ—¶)')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # åˆ›å»ºæ—¥å¿—ç®¡ç†å™¨
    log_manager = LogManager(args.log_dir)
    
    # æ‰§è¡Œå‘½ä»¤
    if args.command == 'list':
        log_manager.show_log_summary()
    elif args.command == 'view':
        log_manager.view_log_file(args.filename, args.lines, args.follow, args.level)
    elif args.command == 'search':
        log_manager.search_logs(args.query, args.files, args.case_sensitive)
    elif args.command == 'clean':
        log_manager.clean_old_logs(args.days, not args.execute)
    elif args.command == 'analyze':
        log_manager.analyze_logs(args.filename, args.hours)

if __name__ == '__main__':
    main()
